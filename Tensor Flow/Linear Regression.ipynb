{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset():\n",
    "    x_data = np.linspace(0,2, 100)\n",
    "    y_data = 1.5* x_data + np.random.randn(*x_data.shape)*0.2 +0.5\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.02020202, 0.04040404, 0.06060606, 0.08080808,\n",
       "       0.1010101 , 0.12121212, 0.14141414, 0.16161616, 0.18181818,\n",
       "       0.2020202 , 0.22222222, 0.24242424, 0.26262626, 0.28282828,\n",
       "       0.3030303 , 0.32323232, 0.34343434, 0.36363636, 0.38383838,\n",
       "       0.4040404 , 0.42424242, 0.44444444, 0.46464646, 0.48484848,\n",
       "       0.50505051, 0.52525253, 0.54545455, 0.56565657, 0.58585859,\n",
       "       0.60606061, 0.62626263, 0.64646465, 0.66666667, 0.68686869,\n",
       "       0.70707071, 0.72727273, 0.74747475, 0.76767677, 0.78787879,\n",
       "       0.80808081, 0.82828283, 0.84848485, 0.86868687, 0.88888889,\n",
       "       0.90909091, 0.92929293, 0.94949495, 0.96969697, 0.98989899,\n",
       "       1.01010101, 1.03030303, 1.05050505, 1.07070707, 1.09090909,\n",
       "       1.11111111, 1.13131313, 1.15151515, 1.17171717, 1.19191919,\n",
       "       1.21212121, 1.23232323, 1.25252525, 1.27272727, 1.29292929,\n",
       "       1.31313131, 1.33333333, 1.35353535, 1.37373737, 1.39393939,\n",
       "       1.41414141, 1.43434343, 1.45454545, 1.47474747, 1.49494949,\n",
       "       1.51515152, 1.53535354, 1.55555556, 1.57575758, 1.5959596 ,\n",
       "       1.61616162, 1.63636364, 1.65656566, 1.67676768, 1.6969697 ,\n",
       "       1.71717172, 1.73737374, 1.75757576, 1.77777778, 1.7979798 ,\n",
       "       1.81818182, 1.83838384, 1.85858586, 1.87878788, 1.8989899 ,\n",
       "       1.91919192, 1.93939394, 1.95959596, 1.97979798, 2.        ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6393712 , 0.68037822, 0.3601892 , 0.59576658, 0.71870376,\n",
       "       0.64589575, 0.76207235, 0.884577  , 0.46696085, 0.47216417,\n",
       "       0.86035762, 0.78956762, 0.97339623, 0.94068412, 1.17486098,\n",
       "       1.03259745, 1.04690475, 0.92488289, 0.83314911, 1.09222144,\n",
       "       1.74645295, 1.19819004, 1.13117364, 1.46989146, 1.21002718,\n",
       "       1.354706  , 0.96961348, 1.45554783, 1.23480952, 1.23365687,\n",
       "       1.26123059, 1.51542956, 1.57671619, 1.41798955, 1.61246519,\n",
       "       1.79257779, 1.59999492, 1.59848967, 1.67026056, 1.79537835,\n",
       "       1.98741957, 1.74571355, 1.53922518, 1.98756733, 1.85167632,\n",
       "       1.71984754, 1.63760502, 1.82099272, 1.80232282, 1.77224496,\n",
       "       2.1090502 , 1.89814881, 2.09859797, 1.99716581, 2.22792035,\n",
       "       2.19029533, 1.82399845, 2.13063919, 2.20290045, 2.20807483,\n",
       "       2.5045957 , 2.67243996, 2.22863347, 2.26500997, 2.0198373 ,\n",
       "       2.60261018, 2.40914271, 2.40921159, 2.97424939, 2.64507094,\n",
       "       2.73059632, 2.97075118, 2.54801506, 2.21103647, 2.88418739,\n",
       "       3.21826725, 3.00378454, 2.88443066, 2.70830639, 3.32166726,\n",
       "       2.98283271, 2.42546316, 3.01714762, 3.08473434, 2.97675711,\n",
       "       3.29993163, 3.01811553, 3.072942  , 2.90293681, 3.27405643,\n",
       "       3.32034339, 3.44403253, 3.0077311 , 3.04274292, 3.59348008,\n",
       "       3.54433607, 3.33479419, 3.40348065, 3.34062248, 3.5645799 ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression():\n",
    "    x = tf.placeholder(tf.float32, shape=(None,), name='x')\n",
    "    y = tf.placeholder(tf.float32, shape=(None, ), name='y')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class variable_scope in module tensorflow.python.ops.variable_scope:\n",
      "\n",
      "class variable_scope(builtins.object)\n",
      " |  variable_scope(name_or_scope, default_name=None, values=None, initializer=None, regularizer=None, caching_device=None, partitioner=None, custom_getter=None, reuse=None, dtype=None, use_resource=None, constraint=None, auxiliary_name_scope=True)\n",
      " |  \n",
      " |  A context manager for defining ops that creates variables (layers).\n",
      " |  \n",
      " |  This context manager validates that the (optional) `values` are from the same\n",
      " |  graph, ensures that graph is the default graph, and pushes a name scope and a\n",
      " |  variable scope.\n",
      " |  \n",
      " |  If `name_or_scope` is not None, it is used as is. If `name_or_scope` is None,\n",
      " |  then `default_name` is used.  In that case, if the same name has been\n",
      " |  previously used in the same scope, it will be made unique by appending `_N`\n",
      " |  to it.\n",
      " |  \n",
      " |  Variable scope allows you to create new variables and to share already created\n",
      " |  ones while providing checks to not create or share by accident. For details,\n",
      " |  see the [Variable Scope How To](https://tensorflow.org/guide/variables), here\n",
      " |  we present only a few basic examples.\n",
      " |  \n",
      " |  Simple example of how to create a new variable:\n",
      " |  \n",
      " |  ```python\n",
      " |  with tf.variable_scope(\"foo\"):\n",
      " |      with tf.variable_scope(\"bar\"):\n",
      " |          v = tf.get_variable(\"v\", [1])\n",
      " |          assert v.name == \"foo/bar/v:0\"\n",
      " |  ```\n",
      " |  \n",
      " |  Simple example of how to reenter a premade variable scope safely:\n",
      " |  \n",
      " |  ```python\n",
      " |  with tf.variable_scope(\"foo\") as vs:\n",
      " |    pass\n",
      " |  \n",
      " |  # Re-enter the variable scope.\n",
      " |  with tf.variable_scope(vs,\n",
      " |                         auxiliary_name_scope=False) as vs1:\n",
      " |    # Restore the original name_scope.\n",
      " |    with tf.name_scope(vs1.original_name_scope):\n",
      " |        v = tf.get_variable(\"v\", [1])\n",
      " |        assert v.name == \"foo/v:0\"\n",
      " |        c = tf.constant([1], name=\"c\")\n",
      " |        assert c.name == \"foo/c:0\"\n",
      " |  ```\n",
      " |  \n",
      " |  Basic example of sharing a variable AUTO_REUSE:\n",
      " |  \n",
      " |  ```python\n",
      " |  def foo():\n",
      " |    with tf.variable_scope(\"foo\", reuse=tf.AUTO_REUSE):\n",
      " |      v = tf.get_variable(\"v\", [1])\n",
      " |    return v\n",
      " |  \n",
      " |  v1 = foo()  # Creates v.\n",
      " |  v2 = foo()  # Gets the same, existing v.\n",
      " |  assert v1 == v2\n",
      " |  ```\n",
      " |  \n",
      " |  Basic example of sharing a variable with reuse=True:\n",
      " |  \n",
      " |  ```python\n",
      " |  with tf.variable_scope(\"foo\"):\n",
      " |      v = tf.get_variable(\"v\", [1])\n",
      " |  with tf.variable_scope(\"foo\", reuse=True):\n",
      " |      v1 = tf.get_variable(\"v\", [1])\n",
      " |  assert v1 == v\n",
      " |  ```\n",
      " |  \n",
      " |  Sharing a variable by capturing a scope and setting reuse:\n",
      " |  \n",
      " |  ```python\n",
      " |  with tf.variable_scope(\"foo\") as scope:\n",
      " |      v = tf.get_variable(\"v\", [1])\n",
      " |      scope.reuse_variables()\n",
      " |      v1 = tf.get_variable(\"v\", [1])\n",
      " |  assert v1 == v\n",
      " |  ```\n",
      " |  \n",
      " |  To prevent accidental sharing of variables, we raise an exception when getting\n",
      " |  an existing variable in a non-reusing scope.\n",
      " |  \n",
      " |  ```python\n",
      " |  with tf.variable_scope(\"foo\"):\n",
      " |      v = tf.get_variable(\"v\", [1])\n",
      " |      v1 = tf.get_variable(\"v\", [1])\n",
      " |      #  Raises ValueError(\"... v already exists ...\").\n",
      " |  ```\n",
      " |  \n",
      " |  Similarly, we raise an exception when trying to get a variable that does not\n",
      " |  exist in reuse mode.\n",
      " |  \n",
      " |  ```python\n",
      " |  with tf.variable_scope(\"foo\", reuse=True):\n",
      " |      v = tf.get_variable(\"v\", [1])\n",
      " |      #  Raises ValueError(\"... v does not exists ...\").\n",
      " |  ```\n",
      " |  \n",
      " |  Note that the `reuse` flag is inherited: if we open a reusing scope, then all\n",
      " |  its sub-scopes become reusing as well.\n",
      " |  \n",
      " |  A note about name scoping: Setting `reuse` does not impact the naming of other\n",
      " |  ops such as mult. See related discussion on\n",
      " |  [github#6189](https://github.com/tensorflow/tensorflow/issues/6189)\n",
      " |  \n",
      " |  Note that up to and including version 1.0, it was allowed (though explicitly\n",
      " |  discouraged) to pass False to the reuse argument, yielding undocumented\n",
      " |  behaviour slightly different from None. Starting at 1.1.0 passing None and\n",
      " |  False as reuse has exactly the same effect.\n",
      " |  \n",
      " |  A note about using variable scopes in multi-threaded environment: Variable\n",
      " |  scopes are thread local, so one thread will not see another thread's current\n",
      " |  scope. Also, when using `default_name`, unique scopes names are also generated\n",
      " |  only on a per thread basis. If the same name was used within a different\n",
      " |  thread, that doesn't prevent a new thread from creating the same scope.\n",
      " |  However, the underlying variable store is shared across threads (within the\n",
      " |  same graph). As such, if another thread tries to create a new variable with\n",
      " |  the same name as a variable created by a previous thread, it will fail unless\n",
      " |  reuse is True.\n",
      " |  \n",
      " |  Further, each thread starts with an empty variable scope. So if you wish to\n",
      " |  preserve name prefixes from a scope from the main thread, you should capture\n",
      " |  the main thread's scope and re-enter it in each thread. For e.g.\n",
      " |  \n",
      " |  ```\n",
      " |  main_thread_scope = variable_scope.get_variable_scope()\n",
      " |  \n",
      " |  # Thread's target function:\n",
      " |  def thread_target_fn(captured_scope):\n",
      " |    with variable_scope.variable_scope(captured_scope):\n",
      " |      # .... regular code for this thread\n",
      " |  \n",
      " |  \n",
      " |  thread = threading.Thread(target=thread_target_fn, args=(main_thread_scope,))\n",
      " |  ```\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |  \n",
      " |  __exit__(self, type_arg, value_arg, traceback_arg)\n",
      " |  \n",
      " |  __init__(self, name_or_scope, default_name=None, values=None, initializer=None, regularizer=None, caching_device=None, partitioner=None, custom_getter=None, reuse=None, dtype=None, use_resource=None, constraint=None, auxiliary_name_scope=True)\n",
      " |      Initialize the context manager.\n",
      " |      \n",
      " |      Args:\n",
      " |        name_or_scope: `string` or `VariableScope`: the scope to open.\n",
      " |        default_name: The default name to use if the `name_or_scope` argument is\n",
      " |          `None`, this name will be uniquified. If name_or_scope is provided it\n",
      " |          won't be used and therefore it is not required and can be None.\n",
      " |        values: The list of `Tensor` arguments that are passed to the op function.\n",
      " |        initializer: default initializer for variables within this scope.\n",
      " |        regularizer: default regularizer for variables within this scope.\n",
      " |        caching_device: default caching device for variables within this scope.\n",
      " |        partitioner: default partitioner for variables within this scope.\n",
      " |        custom_getter: default custom getter for variables within this scope.\n",
      " |        reuse: `True`, None, or tf.AUTO_REUSE; if `True`, we go into reuse mode\n",
      " |          for this scope as well as all sub-scopes; if tf.AUTO_REUSE, we create\n",
      " |          variables if they do not exist, and return them otherwise; if None, we\n",
      " |          inherit the parent scope's reuse flag. When eager execution is enabled,\n",
      " |          new variables are always created unless an EagerVariableStore or\n",
      " |          template is currently active.\n",
      " |        dtype: type of variables created in this scope (defaults to the type\n",
      " |          in the passed scope, or inherited from parent scope).\n",
      " |        use_resource: If False, all variables will be regular Variables. If True,\n",
      " |          experimental ResourceVariables with well-defined semantics will be used\n",
      " |          instead. Defaults to False (will later change to True). When eager\n",
      " |          execution is enabled this argument is always forced to be True.\n",
      " |        constraint: An optional projection function to be applied to the variable\n",
      " |          after being updated by an `Optimizer` (e.g. used to implement norm\n",
      " |          constraints or value constraints for layer weights). The function must\n",
      " |          take as input the unprojected Tensor representing the value of the\n",
      " |          variable and return the Tensor for the projected value\n",
      " |          (which must have the same shape). Constraints are not safe to\n",
      " |          use when doing asynchronous distributed training.\n",
      " |        auxiliary_name_scope: If `True`, we create an auxiliary name scope with\n",
      " |          the scope. If `False`, we don't create it. Note that the argument is\n",
      " |          not inherited, and it only takes effect for once when creating. You\n",
      " |          should only use it for re-entering a premade variable scope.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A scope that can be captured and reused.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: when trying to reuse within a create scope, or create within\n",
      " |          a reuse scope.\n",
      " |        TypeError: when the types of some arguments are not appropriate.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.variable_scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
